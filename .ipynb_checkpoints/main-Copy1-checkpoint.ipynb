{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_=True\n",
    "args = dict(mode='rgb', \n",
    "            train=True, \n",
    "            comp_info='charades_PDAN', \n",
    "            rgb_model_file=None, \n",
    "            flow_model_file=None, \n",
    "            gpu='0', dataset='charades', \n",
    "            rgb_root='../I3D_Feature_Extraction_resnet-main/output/Charades_v1_480', \n",
    "            flow_root='no_root', type='original', lr='0.0001', \n",
    "            epoch='100', model='PDAN', APtype='map', randomseed='False', load_model='False', \n",
    "            batch_size='32', num_channel='512', run_mode='False', feat='False',\n",
    "            early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_[:, 0:1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 47])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_[:, 0:1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 157, 47])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_SEED!!!: 0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# sys.path.append('/data/stars/user/rdai/PhD_work/Graph_net')\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# set random seed\n",
    "if args['randomseed']==\"False\":\n",
    "    SEED = 0\n",
    "elif args['randomseed']==\"True\":\n",
    "    SEED = random.randint(1, 100000)\n",
    "else:\n",
    "    SEED = int(args['randomseed'])\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print('Random_SEED!!!:', SEED)\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "if str(args['APtype']) == 'wap':\n",
    "    pass\n",
    "    #from wapmeter import APMeter\n",
    "elif str(args['APtype']) == 'map':\n",
    "    from apmeter import APMeter\n",
    "\n",
    "\n",
    "batch_size = int(args['batch_size'])\n",
    "\n",
    "\n",
    "from charades_i3d_per_video import MultiThumos as Dataset\n",
    "from charades_i3d_per_video import mt_collate_fn as collate_fn\n",
    "\n",
    "train_split = './data/charades.json'\n",
    "test_split = './data/charades.json'\n",
    "# print('load feature from:', args.rgb_root)\n",
    "# rgb_root = '/Path/to/charades_feat_rgb'\n",
    "# skeleton_root = '/Path/to/charades_feat_pose'\n",
    "# flow_root = '/Path/to/charades_feat_flow'\n",
    "# rgb_of=[rgb_root,flow_root]\n",
    "classes = 157\n",
    "\n",
    "\n",
    "def load_data(train_split, val_split, root):\n",
    "    # Load Data\n",
    "    print('load data', root)\n",
    "    if len(train_split) > 0:\n",
    "        if str(args['feat']) == '2d':\n",
    "            dataset = Dataset(train_split, 'training', root, batch_size, classes, int(args['pool_step']))\n",
    "        else:\n",
    "            dataset = Dataset(train_split, 'training', root, batch_size, classes)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8,\n",
    "                                                 pin_memory=True, collate_fn=collate_fn)\n",
    "        dataloader.root = root\n",
    "    else:\n",
    "\n",
    "        dataset = None\n",
    "        dataloader = None\n",
    "\n",
    "    if str(args['feat']) == '2d':\n",
    "        val_dataset = Dataset(val_split, 'testing', root, batch_size, classes, int(args['pool_step']))\n",
    "    else:\n",
    "        val_dataset = Dataset(val_split, 'testing', root, batch_size, classes)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=2,\n",
    "                                                 pin_memory=True, collate_fn=collate_fn)\n",
    "    val_dataloader.root = root\n",
    "\n",
    "    dataloaders = {'train': dataloader, 'val': val_dataloader}\n",
    "    datasets = {'train': dataset, 'val': val_dataset}\n",
    "    return dataloaders, datasets\n",
    "\n",
    "\n",
    "# train the model\n",
    "def run(models, criterion, num_epochs=50):\n",
    "    since = time.time()\n",
    "    \n",
    "    if args['early_stopping']:\n",
    "        print('INFO: Initializing early stopping')\n",
    "        early_stopping = EarlyStopping(patience=20)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        probs = []\n",
    "#         for model, gpu, dataloader, optimizer, sched, model_file in models:\n",
    "#             train_map, train_loss = train_step(model, gpu, optimizer, dataloader['train'], epoch)\n",
    "#             prob_val, val_loss, val_map = val_step(model, gpu, dataloader['val'], epoch)\n",
    "#             probs.append(prob_val)\n",
    "#             sched.step(val_loss)\n",
    "#             print('--------Im in for loop')\n",
    "        model, gpu, dataloader, optimizer, sched, model_file = models[0]\n",
    "        train_map, train_loss = train_step(model, gpu, optimizer, dataloader['train'], epoch)\n",
    "        prob_val, val_loss, val_map = val_step(model, gpu, dataloader['val'], epoch)\n",
    "        probs.append(prob_val)\n",
    "        sched.step(val_loss)\n",
    "        \n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_map, epoch)\n",
    "        writer.add_scalar('Accuracy/val',  val_map, epoch)\n",
    "        \n",
    "        print(val_loss.item(), 'val_loss.item()')\n",
    "        #TODO: fix\n",
    "        if args['early_stopping']:\n",
    "            early_stopping(val_loss.item())\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "def eval_model(model, dataloader, baseline=False):\n",
    "    results = {}\n",
    "    for data in dataloader:\n",
    "        other = data[3]\n",
    "        \n",
    "        outputs, loss, probs, _ = run_network(model[0][0], data, 0, baseline)\n",
    "        fps = outputs.size()[1] / other[1][0]\n",
    "\n",
    "        results[other[0][0]] = (outputs.data.cpu().numpy()[0].tolist(), probs.data.cpu().numpy()[0].tolist(), data[2].numpy()[0].tolist(), float(fps))\n",
    "#         results[other[0][0]] = (outputs.data.cpu().numpy()[0], probs.data.cpu().numpy()[0], data[2].numpy()[0], fps)\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_network(model, data, gpu, epoch=0, baseline=False):\n",
    "    inputs, mask, labels, other = data\n",
    "    # wrap them in Variable\n",
    "    inputs = Variable(inputs.cuda(gpu))\n",
    "    mask = Variable(mask.cuda(gpu))\n",
    "    labels = Variable(labels.cuda(gpu))\n",
    "\n",
    "    mask_list = torch.sum(mask, 1)\n",
    "    mask_new = np.zeros((mask.size()[0], classes, mask.size()[1]))\n",
    "    for i in range(mask.size()[0]):\n",
    "        mask_new[i, :, :int(mask_list[i])] = np.ones((classes, int(mask_list[i])))\n",
    "#     print(mask_new)\n",
    "#     print(np.shape(mask_new))\n",
    "#     print(mask[0, 0:1, :], \"\\n\\n\")\n",
    "    mask_new = torch.from_numpy(mask_new).float()\n",
    "    mask_new = Variable(mask_new.cuda(gpu))\n",
    "    \n",
    "#     global mask_\n",
    "#     mask_ = mask_new\n",
    "#     return 0\n",
    "\n",
    "    inputs = inputs.squeeze(3).squeeze(3)\n",
    "    \n",
    "    activation = model(inputs, mask_new)\n",
    "    #TODO\n",
    "#     activation = model[0](inputs, mask_new)\n",
    "\n",
    "    \n",
    "    outputs_final = activation\n",
    "\n",
    "    #print(\"outputs_final\",outputs_final.size())\n",
    "    outputs_final = outputs_final[-1]\n",
    "    #print(\"outputs_final\",outputs_final.size())\n",
    "    outputs_final = outputs_final.permute(0, 2, 1)  \n",
    "    probs_f = torch.sigmoid(outputs_final) * mask.unsqueeze(2)\n",
    "    loss_f = F.binary_cross_entropy_with_logits(outputs_final, labels, size_average=False)\n",
    "    loss_f = torch.sum(loss_f) / torch.sum(mask)  \n",
    "\n",
    "    loss = loss_f \n",
    "\n",
    "    corr = torch.sum(mask)\n",
    "    tot = torch.sum(mask)\n",
    "\n",
    "    return outputs_final, loss, probs_f, corr / tot\n",
    "\n",
    "\n",
    "def train_step(model, gpu, optimizer, dataloader, epoch):\n",
    "    model.train(True)\n",
    "    tot_loss = 0.0\n",
    "    error = 0.0\n",
    "    num_iter = 0.\n",
    "    apm = APMeter()\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        num_iter += 1\n",
    "\n",
    "        outputs, loss, probs, err = run_network(model, data, gpu, epoch)\n",
    "        apm.add(probs.data.cpu().numpy()[0], data[2].numpy()[0])\n",
    "        error += err.data\n",
    "        tot_loss += loss.data\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if args['APtype'] == 'wap':\n",
    "        train_map = 100 * apm.value()\n",
    "    else:\n",
    "        train_map = 100 * apm.value().mean()\n",
    "    print('train-map:', train_map)\n",
    "    apm.reset()\n",
    "\n",
    "    epoch_loss = tot_loss / num_iter\n",
    "\n",
    "    return train_map, epoch_loss\n",
    "\n",
    "\n",
    "def val_step(model, gpu, dataloader, epoch):\n",
    "    model.train(False)\n",
    "    apm = APMeter()\n",
    "    tot_loss = 0.0\n",
    "    error = 0.0\n",
    "    num_iter = 0.\n",
    "    num_preds = 0\n",
    "\n",
    "    full_probs = {}\n",
    "\n",
    "    # Iterate over data.\n",
    "    for data in dataloader:\n",
    "        num_iter += 1\n",
    "        other = data[3]\n",
    "\n",
    "        outputs, loss, probs, err = run_network(model, data, gpu, epoch)\n",
    "\n",
    "        apm.add(probs.data.cpu().numpy()[0], data[2].numpy()[0])\n",
    "\n",
    "        error += err.data\n",
    "        tot_loss += loss.data\n",
    "\n",
    "        probs = probs.squeeze()\n",
    "\n",
    "        full_probs[other[0][0]] = probs.data.cpu().numpy().T\n",
    "\n",
    "    epoch_loss = tot_loss / num_iter\n",
    "\n",
    "\n",
    "    val_map = torch.sum(100 * apm.value()) / torch.nonzero(100 * apm.value()).size()[0]\n",
    "    print('val-map:', val_map)\n",
    "#     print(100 * apm.value())\n",
    "    apm.reset()\n",
    "\n",
    "    return full_probs, epoch_loss, val_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32\n",
      "cuda_avail True\n",
      "load data ../I3D_Feature_Extraction_resnet-main/output/Charades_v1_480\n",
      "split!!!! training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9848/9848 [00:07<00:00, 1329.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split!!!! testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9848/9848 [00:01<00:00, 5323.19it/s]\n",
      "C:\\Users\\David\\Desktop\\har\\PDAN\\PDAN.py:98: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.key_conv.weight, mode='fan_out')\n",
      "C:\\Users\\David\\Desktop\\har\\PDAN\\PDAN.py:99: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.value_conv.weight, mode='fan_out')\n",
      "C:\\Users\\David\\Desktop\\har\\PDAN\\PDAN.py:100: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.query_conv.weight, mode='fan_out')\n",
      "C:\\Users\\David\\Desktop\\har\\PDAN\\PDAN.py:101: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.rel_t, 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are processing PDAN_original\n",
      "pytorch_total_params 6382749\n",
      "stage: 1 block: 5 num_channel: 512 input_channnel: 2048 num_classes: 157\n",
      "pytorch_total_params 6382749\n",
      "num_channel: 512 input_channnel: 2048 num_classes: 157\n",
      "lr 0.0001\n",
      "<class 'torch.nn.parallel.data_parallel.DataParallel'> type model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "\n",
    "args['model'] = 'PDAN'\n",
    "print('batch_size:', batch_size)\n",
    "print('cuda_avail', torch.cuda.is_available())\n",
    "\n",
    "dataloaders, datasets = load_data(train_split, test_split, args['rgb_root'])\n",
    "\n",
    "if args['train']:\n",
    "    num_channel = args['num_channel']\n",
    "    input_channnel = 2048\n",
    "\n",
    "    num_classes = classes\n",
    "    mid_channel=int(args['num_channel'])\n",
    "\n",
    "    if args['model']==\"PDAN\":\n",
    "        print(\"you are processing PDAN_original\")\n",
    "        from PDAN import PDAN\n",
    "        # rgb_model = Net(mid_channel, input_channnel, classes)\n",
    "        stage=1\n",
    "        block=5\n",
    "        num_channel=512\n",
    "        input_channnel=2048\n",
    "        num_classes=classes\n",
    "        rgb_model = PDAN(stage, block, num_channel, input_channnel, num_classes)\n",
    "        pytorch_total_params = sum(p.numel() for p in rgb_model.parameters() if p.requires_grad)\n",
    "        print('pytorch_total_params', pytorch_total_params)\n",
    "        #exit()\n",
    "        print ('stage:', stage, 'block:', block, 'num_channel:', num_channel, 'input_channnel:', input_channnel,\n",
    "               'num_classes:', num_classes)\n",
    "\n",
    "\n",
    "    rgb_model=torch.nn.DataParallel(rgb_model)\n",
    "\n",
    "    if args['load_model']!= \"False\":\n",
    "        state_dict = torch.load(str(args['load_model']))\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in rgb_model.parameters() if p.requires_grad)\n",
    "    print('pytorch_total_params', pytorch_total_params)\n",
    "    print('num_channel:', num_channel, 'input_channnel:', input_channnel,'num_classes:', num_classes)\n",
    "    rgb_model.cuda()\n",
    "\n",
    "    criterion = nn.NLLLoss(reduce=False)\n",
    "    lr = float(args['lr'])\n",
    "    print('lr', lr)\n",
    "    optimizer = optim.Adam(rgb_model.parameters(), lr=lr)\n",
    "    lr_sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=8, verbose=True)\n",
    "    print(type(rgb_model), 'type model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): PDAN(\n",
       "    (stage1): SSPDAN(\n",
       "      (conv_1x1): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "      (layers): ModuleList(\n",
       "        (0): PDAN_Block(\n",
       "          (conv_attention): DAL(\n",
       "            (key_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (query_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (value_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (conv_1x1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (1): PDAN_Block(\n",
       "          (conv_attention): DAL(\n",
       "            (key_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (query_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (value_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (conv_1x1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (2): PDAN_Block(\n",
       "          (conv_attention): DAL(\n",
       "            (key_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (query_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (value_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (conv_1x1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (3): PDAN_Block(\n",
       "          (conv_attention): DAL(\n",
       "            (key_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (query_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (value_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (conv_1x1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (4): PDAN_Block(\n",
       "          (conv_attention): DAL(\n",
       "            (key_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (query_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            (value_conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "          (conv_1x1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (conv_out): Conv1d(512, 157, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (stages): ModuleList()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\David\\Desktop\\har\\PDAN\\apmeter.py:108: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  rg = torch.range(1, self.scores.size(0)).float()\n",
      "C:\\Users\\David\\Desktop\\har\\PDAN\\apmeter.py:136: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  ap[k] = precision[truth.byte()].sum() / max(truth.sum(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-map: tensor(2.1128)\n",
      "val-map: tensor(5.0986)\n",
      "15.466021537780762 val_loss.item()\n",
      "Epoch 1/99\n",
      "----------\n",
      "train-map: tensor(4.6105)\n",
      "val-map: tensor(6.6886)\n",
      "14.98080062866211 val_loss.item()\n",
      "Epoch 2/99\n",
      "----------\n",
      "train-map: tensor(5.4792)\n",
      "val-map: tensor(7.6181)\n",
      "15.067852020263672 val_loss.item()\n",
      "Epoch 3/99\n",
      "----------\n",
      "train-map: tensor(7.5792)\n",
      "val-map: tensor(8.1531)\n",
      "14.860237121582031 val_loss.item()\n",
      "Epoch 4/99\n",
      "----------\n",
      "train-map: tensor(7.8553)\n",
      "val-map: tensor(8.7103)\n",
      "14.721099853515625 val_loss.item()\n",
      "Epoch 5/99\n",
      "----------\n",
      "train-map: tensor(8.9429)\n",
      "val-map: tensor(9.0770)\n",
      "14.558691024780273 val_loss.item()\n",
      "Epoch 6/99\n",
      "----------\n",
      "train-map: tensor(10.4582)\n",
      "val-map: tensor(9.3920)\n",
      "14.588655471801758 val_loss.item()\n",
      "Epoch 7/99\n",
      "----------\n",
      "train-map: tensor(11.4889)\n",
      "val-map: tensor(9.6692)\n",
      "14.410964012145996 val_loss.item()\n",
      "Epoch 8/99\n",
      "----------\n",
      "train-map: tensor(9.9026)\n",
      "val-map: tensor(9.8736)\n",
      "14.64443588256836 val_loss.item()\n",
      "Epoch 9/99\n",
      "----------\n",
      "train-map: tensor(12.5856)\n",
      "val-map: tensor(10.0268)\n",
      "14.372509002685547 val_loss.item()\n",
      "Epoch 10/99\n",
      "----------\n",
      "train-map: tensor(12.5926)\n",
      "val-map: tensor(10.0515)\n",
      "14.441746711730957 val_loss.item()\n",
      "Epoch 11/99\n",
      "----------\n",
      "train-map: tensor(14.1863)\n",
      "val-map: tensor(10.1849)\n",
      "14.45200252532959 val_loss.item()\n",
      "Epoch 12/99\n",
      "----------\n",
      "train-map: tensor(13.2257)\n",
      "val-map: tensor(10.3118)\n",
      "14.610413551330566 val_loss.item()\n",
      "Epoch 13/99\n",
      "----------\n",
      "train-map: tensor(16.9235)\n",
      "val-map: tensor(10.4688)\n",
      "14.549148559570312 val_loss.item()\n",
      "Epoch 14/99\n",
      "----------\n",
      "train-map: tensor(19.7118)\n",
      "val-map: tensor(10.3778)\n",
      "14.874777793884277 val_loss.item()\n",
      "Epoch 15/99\n",
      "----------\n",
      "train-map: tensor(21.4756)\n",
      "val-map: tensor(10.4641)\n",
      "14.611352920532227 val_loss.item()\n",
      "Epoch 16/99\n",
      "----------\n",
      "train-map: tensor(16.0824)\n",
      "val-map: tensor(10.5416)\n",
      "14.689932823181152 val_loss.item()\n",
      "Epoch 17/99\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run([(rgb_model, 0, dataloaders, optimizer, lr_sched, args['comp_info'])], criterion, num_epochs=int(args['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = eval_model([(rgb_model, 0, dataloaders, optimizer, lr_sched, args['comp_info'])], dataloaders['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_:\n",
    "    with open('./output/results_less_param.txt', 'w') as file:\n",
    "        file.write(json.dumps(results)) # use `json.loads` to do the reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_:\n",
    "    torch.save(rgb_model, './output/rgb_model_less_param.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
