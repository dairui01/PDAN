{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_=True\n",
    "args = dict(mode='rgb', \n",
    "            train=True, \n",
    "            comp_info='charades_PDAN', \n",
    "            rgb_model_file=None, \n",
    "            flow_model_file=None, \n",
    "            gpu='0', dataset='charades', \n",
    "            rgb_root='../I3D_Feature_Extraction_resnet-main/output/Charades_v1_480', \n",
    "            flow_root='no_root', type='original', lr='0.0001', \n",
    "            epoch='100', model='PDAN', APtype='map', randomseed='False', load_model='False', \n",
    "            batch_size='32', num_channel='512', run_mode='False', feat='False',\n",
    "            early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_SEED!!!: 0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# sys.path.append('/data/stars/user/rdai/PhD_work/Graph_net')\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# set random seed\n",
    "if args['randomseed']==\"False\":\n",
    "    SEED = 0\n",
    "elif args['randomseed']==\"True\":\n",
    "    SEED = random.randint(1, 100000)\n",
    "else:\n",
    "    SEED = int(args['randomseed'])\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print('Random_SEED!!!:', SEED)\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "if str(args['APtype']) == 'wap':\n",
    "    pass\n",
    "    #from wapmeter import APMeter\n",
    "elif str(args['APtype']) == 'map':\n",
    "    from apmeter import APMeter\n",
    "\n",
    "\n",
    "batch_size = int(args['batch_size'])\n",
    "\n",
    "\n",
    "from charades_i3d_per_video import MultiThumos as Dataset\n",
    "from charades_i3d_per_video import mt_collate_fn as collate_fn\n",
    "\n",
    "train_split = './data/charades.json'\n",
    "test_split = './data/charades.json'\n",
    "# print('load feature from:', args.rgb_root)\n",
    "# rgb_root = '/Path/to/charades_feat_rgb'\n",
    "# skeleton_root = '/Path/to/charades_feat_pose'\n",
    "# flow_root = '/Path/to/charades_feat_flow'\n",
    "# rgb_of=[rgb_root,flow_root]\n",
    "classes = 157\n",
    "\n",
    "\n",
    "def load_data(train_split, val_split, root):\n",
    "    # Load Data\n",
    "    print('load data', root)\n",
    "    if len(train_split) > 0:\n",
    "        if str(args['feat']) == '2d':\n",
    "            dataset = Dataset(train_split, 'training', root, batch_size, classes, int(args['pool_step']))\n",
    "        else:\n",
    "            dataset = Dataset(train_split, 'training', root, batch_size, classes)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8,\n",
    "                                                 pin_memory=True, collate_fn=collate_fn)\n",
    "        dataloader.root = root\n",
    "    else:\n",
    "\n",
    "        dataset = None\n",
    "        dataloader = None\n",
    "\n",
    "    if str(args['feat']) == '2d':\n",
    "        val_dataset = Dataset(val_split, 'testing', root, batch_size, classes, int(args['pool_step']))\n",
    "    else:\n",
    "        val_dataset = Dataset(val_split, 'testing', root, batch_size, classes)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=2,\n",
    "                                                 pin_memory=True, collate_fn=collate_fn)\n",
    "    val_dataloader.root = root\n",
    "\n",
    "    dataloaders = {'train': dataloader, 'val': val_dataloader}\n",
    "    datasets = {'train': dataset, 'val': val_dataset}\n",
    "    return dataloaders, datasets\n",
    "\n",
    "\n",
    "# train the model\n",
    "def run(models, criterion, num_epochs=50):\n",
    "    since = time.time()\n",
    "    \n",
    "    if args['early_stopping']:\n",
    "        print('INFO: Initializing early stopping')\n",
    "        early_stopping = EarlyStopping()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        probs = []\n",
    "#         for model, gpu, dataloader, optimizer, sched, model_file in models:\n",
    "#             train_map, train_loss = train_step(model, gpu, optimizer, dataloader['train'], epoch)\n",
    "#             prob_val, val_loss, val_map = val_step(model, gpu, dataloader['val'], epoch)\n",
    "#             probs.append(prob_val)\n",
    "#             sched.step(val_loss)\n",
    "#             print('--------Im in for loop')\n",
    "        model, gpu, dataloader, optimizer, sched, model_file = models[0]\n",
    "        train_map, train_loss = train_step(model, gpu, optimizer, dataloader['train'], epoch)\n",
    "        prob_val, val_loss, val_map = val_step(model, gpu, dataloader['val'], epoch)\n",
    "        probs.append(prob_val)\n",
    "        sched.step(val_loss)\n",
    "        \n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_map, epoch)\n",
    "        writer.add_scalar('Accuracy/val',  val_map, epoch)\n",
    "        \n",
    "        print(val_loss.item(), 'val_loss.item()')\n",
    "        #TODO: fix\n",
    "        if args['early_stopping']:\n",
    "            early_stopping(val_loss.item())\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "def eval_model(model, dataloader, baseline=False):\n",
    "    results = {}\n",
    "    for data in dataloader:\n",
    "        other = data[3]\n",
    "        \n",
    "        outputs, loss, probs, _ = run_network(model[0][0], data, 0, baseline)\n",
    "        fps = outputs.size()[1] / other[1][0]\n",
    "\n",
    "        results[other[0][0]] = (outputs.data.cpu().numpy()[0].tolist(), probs.data.cpu().numpy()[0].tolist(), data[2].numpy()[0].tolist(), float(fps))\n",
    "#         results[other[0][0]] = (outputs.data.cpu().numpy()[0], probs.data.cpu().numpy()[0], data[2].numpy()[0], fps)\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_network(model, data, gpu, epoch=0, baseline=False):\n",
    "    inputs, mask, labels, other = data\n",
    "    # wrap them in Variable\n",
    "    inputs = Variable(inputs.cuda(gpu))\n",
    "    mask = Variable(mask.cuda(gpu))\n",
    "    labels = Variable(labels.cuda(gpu))\n",
    "\n",
    "    mask_list = torch.sum(mask, 1)\n",
    "    mask_new = np.zeros((mask.size()[0], classes, mask.size()[1]))\n",
    "    for i in range(mask.size()[0]):\n",
    "        mask_new[i, :, :int(mask_list[i])] = np.ones((classes, int(mask_list[i])))\n",
    "    mask_new = torch.from_numpy(mask_new).float()\n",
    "    mask_new = Variable(mask_new.cuda(gpu))\n",
    "\n",
    "    inputs = inputs.squeeze(3).squeeze(3)\n",
    "    \n",
    "    activation = model(inputs, mask_new)\n",
    "    #TODO\n",
    "#     activation = model[0](inputs, mask_new)\n",
    "\n",
    "    \n",
    "    outputs_final = activation\n",
    "\n",
    "    #print(\"outputs_final\",outputs_final.size())\n",
    "    outputs_final = outputs_final[-1]\n",
    "    #print(\"outputs_final\",outputs_final.size())\n",
    "    outputs_final = outputs_final.permute(0, 2, 1)  \n",
    "    probs_f = torch.sigmoid(outputs_final) * mask.unsqueeze(2)\n",
    "    loss_f = F.binary_cross_entropy_with_logits(outputs_final, labels, size_average=False)\n",
    "    loss_f = torch.sum(loss_f) / torch.sum(mask)  \n",
    "\n",
    "    loss = loss_f \n",
    "\n",
    "    corr = torch.sum(mask)\n",
    "    tot = torch.sum(mask)\n",
    "\n",
    "    return outputs_final, loss, probs_f, corr / tot\n",
    "\n",
    "\n",
    "def train_step(model, gpu, optimizer, dataloader, epoch):\n",
    "    model.train(True)\n",
    "    tot_loss = 0.0\n",
    "    error = 0.0\n",
    "    num_iter = 0.\n",
    "    apm = APMeter()\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        num_iter += 1\n",
    "\n",
    "        outputs, loss, probs, err = run_network(model, data, gpu, epoch)\n",
    "        apm.add(probs.data.cpu().numpy()[0], data[2].numpy()[0])\n",
    "        error += err.data\n",
    "        tot_loss += loss.data\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if args['APtype'] == 'wap':\n",
    "        train_map = 100 * apm.value()\n",
    "    else:\n",
    "        train_map = 100 * apm.value().mean()\n",
    "    print('train-map:', train_map)\n",
    "    apm.reset()\n",
    "\n",
    "    epoch_loss = tot_loss / num_iter\n",
    "\n",
    "    return train_map, epoch_loss\n",
    "\n",
    "\n",
    "def val_step(model, gpu, dataloader, epoch):\n",
    "    model.train(False)\n",
    "    apm = APMeter()\n",
    "    tot_loss = 0.0\n",
    "    error = 0.0\n",
    "    num_iter = 0.\n",
    "    num_preds = 0\n",
    "\n",
    "    full_probs = {}\n",
    "\n",
    "    # Iterate over data.\n",
    "    for data in dataloader:\n",
    "        num_iter += 1\n",
    "        other = data[3]\n",
    "\n",
    "        outputs, loss, probs, err = run_network(model, data, gpu, epoch)\n",
    "\n",
    "        apm.add(probs.data.cpu().numpy()[0], data[2].numpy()[0])\n",
    "\n",
    "        error += err.data\n",
    "        tot_loss += loss.data\n",
    "\n",
    "        probs = probs.squeeze()\n",
    "\n",
    "        full_probs[other[0][0]] = probs.data.cpu().numpy().T\n",
    "\n",
    "    epoch_loss = tot_loss / num_iter\n",
    "\n",
    "\n",
    "    val_map = torch.sum(100 * apm.value()) / torch.nonzero(100 * apm.value()).size()[0]\n",
    "    print('val-map:', val_map)\n",
    "#     print(100 * apm.value())\n",
    "    apm.reset()\n",
    "\n",
    "    return full_probs, epoch_loss, val_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32\n",
      "cuda_avail True\n",
      "load data ../I3D_Feature_Extraction_resnet-main/output/Charades_v1_480\n",
      "split!!!! training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9848/9848 [00:06<00:00, 1485.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split!!!! testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9848/9848 [00:01<00:00, 6422.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are processing PDAN_original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Desktop\\har\\PDAN-main\\PDAN.py:89: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.key_conv.weight, mode='fan_out')\n",
      "C:\\Users\\David\\Desktop\\har\\PDAN-main\\PDAN.py:90: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.value_conv.weight, mode='fan_out')\n",
      "C:\\Users\\David\\Desktop\\har\\PDAN-main\\PDAN.py:91: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.query_conv.weight, mode='fan_out')\n",
      "C:\\Users\\David\\Desktop\\har\\PDAN-main\\PDAN.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.rel_t, 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch_total_params 6382749\n",
      "stage: 1 block: 5 num_channel: 512 input_channnel: 2048 num_classes: 157\n",
      "pytorch_total_params 6382749\n",
      "num_channel: 512 input_channnel: 2048 num_classes: 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.0001\n",
      "<class 'torch.nn.parallel.data_parallel.DataParallel'> type model\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "\n",
    "args['model'] = 'PDAN'\n",
    "print('batch_size:', batch_size)\n",
    "print('cuda_avail', torch.cuda.is_available())\n",
    "\n",
    "dataloaders, datasets = load_data(train_split, test_split, args['rgb_root'])\n",
    "\n",
    "if args['train']:\n",
    "    num_channel = args['num_channel']\n",
    "    input_channnel = 2048\n",
    "\n",
    "    num_classes = classes\n",
    "    mid_channel=int(args['num_channel'])\n",
    "\n",
    "    if args['model']==\"PDAN\":\n",
    "        print(\"you are processing PDAN_original\")\n",
    "        from PDAN import PDAN\n",
    "        # rgb_model = Net(mid_channel, input_channnel, classes)\n",
    "        stage=1\n",
    "        block=5\n",
    "        num_channel=512\n",
    "        input_channnel=2048\n",
    "        num_classes=classes\n",
    "        rgb_model = PDAN(stage, block, num_channel, input_channnel, num_classes)\n",
    "        pytorch_total_params = sum(p.numel() for p in rgb_model.parameters() if p.requires_grad)\n",
    "        print('pytorch_total_params', pytorch_total_params)\n",
    "        #exit()\n",
    "        print ('stage:', stage, 'block:', block, 'num_channel:', num_channel, 'input_channnel:', input_channnel,\n",
    "               'num_classes:', num_classes)\n",
    "\n",
    "\n",
    "    rgb_model=torch.nn.DataParallel(rgb_model)\n",
    "\n",
    "    if args['load_model']!= \"False\":\n",
    "        state_dict = torch.load(str(args['load_model']))\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in rgb_model.parameters() if p.requires_grad)\n",
    "    print('pytorch_total_params', pytorch_total_params)\n",
    "    print('num_channel:', num_channel, 'input_channnel:', input_channnel,'num_classes:', num_classes)\n",
    "    rgb_model.cuda()\n",
    "\n",
    "    criterion = nn.NLLLoss(reduce=False)\n",
    "    lr = float(args['lr'])\n",
    "    print('lr', lr)\n",
    "    optimizer = optim.Adam(rgb_model.parameters(), lr=lr)\n",
    "    lr_sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=8, verbose=True)\n",
    "    print(type(rgb_model), 'type model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\david\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\David\\Desktop\\har\\PDAN-main\\apmeter.py:108: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  rg = torch.range(1, self.scores.size(0)).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-map: tensor(2.0511)\n",
      "val-map: tensor(5.2134)\n",
      "15.3865385055542 val_loss.item()\n",
      "Epoch 1/99\n",
      "----------\n",
      "train-map: tensor(4.5742)\n",
      "val-map: tensor(6.8618)\n",
      "14.983321189880371 val_loss.item()\n",
      "Epoch 2/99\n",
      "----------\n",
      "train-map: tensor(5.8262)\n",
      "val-map: tensor(7.6633)\n",
      "14.848044395446777 val_loss.item()\n",
      "Epoch 3/99\n",
      "----------\n",
      "train-map: tensor(7.1686)\n",
      "val-map: tensor(8.2149)\n",
      "15.007850646972656 val_loss.item()\n",
      "Epoch 4/99\n",
      "----------\n",
      "train-map: tensor(7.3825)\n",
      "val-map: tensor(8.7203)\n",
      "14.72641372680664 val_loss.item()\n",
      "Epoch 5/99\n",
      "----------\n",
      "train-map: tensor(7.5304)\n",
      "val-map: tensor(9.1867)\n",
      "14.554095268249512 val_loss.item()\n",
      "Epoch 6/99\n",
      "----------\n",
      "train-map: tensor(9.6721)\n",
      "val-map: tensor(9.4232)\n",
      "14.4320707321167 val_loss.item()\n",
      "Epoch 7/99\n",
      "----------\n",
      "train-map: tensor(10.5500)\n",
      "val-map: tensor(9.8080)\n",
      "14.430047035217285 val_loss.item()\n",
      "Epoch 8/99\n",
      "----------\n",
      "train-map: tensor(11.7393)\n",
      "val-map: tensor(9.9873)\n",
      "14.394678115844727 val_loss.item()\n",
      "Epoch 9/99\n",
      "----------\n",
      "train-map: tensor(12.6230)\n",
      "val-map: tensor(10.0242)\n",
      "14.61408519744873 val_loss.item()\n",
      "Epoch 10/99\n",
      "----------\n",
      "train-map: tensor(15.0952)\n",
      "val-map: tensor(10.4770)\n",
      "14.437396049499512 val_loss.item()\n",
      "Epoch 11/99\n",
      "----------\n",
      "train-map: tensor(16.0637)\n",
      "val-map: tensor(10.4550)\n",
      "14.40558910369873 val_loss.item()\n",
      "Epoch 12/99\n",
      "----------\n",
      "train-map: tensor(13.9160)\n",
      "val-map: tensor(10.4535)\n",
      "14.521110534667969 val_loss.item()\n",
      "Epoch 13/99\n",
      "----------\n",
      "train-map: tensor(16.6853)\n",
      "val-map: tensor(10.4807)\n",
      "14.550137519836426 val_loss.item()\n",
      "Epoch 14/99\n",
      "----------\n",
      "train-map: tensor(18.2603)\n",
      "val-map: tensor(10.4796)\n",
      "14.523167610168457 val_loss.item()\n",
      "Epoch 15/99\n",
      "----------\n",
      "train-map: tensor(19.5550)\n",
      "val-map: tensor(10.3815)\n",
      "14.537845611572266 val_loss.item()\n",
      "Epoch 16/99\n",
      "----------\n",
      "train-map: tensor(20.7901)\n",
      "val-map: tensor(10.5680)\n",
      "15.018362998962402 val_loss.item()\n",
      "Epoch 17/99\n",
      "----------\n",
      "train-map: tensor(21.3129)\n",
      "val-map: tensor(10.5296)\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0000e-05.\n",
      "14.721110343933105 val_loss.item()\n",
      "Epoch 18/99\n",
      "----------\n",
      "train-map: tensor(22.3712)\n",
      "val-map: tensor(10.6568)\n",
      "14.807480812072754 val_loss.item()\n",
      "Epoch 19/99\n",
      "----------\n",
      "train-map: tensor(25.3419)\n",
      "val-map: tensor(10.4065)\n",
      "14.94958782196045 val_loss.item()\n",
      "Epoch 20/99\n",
      "----------\n",
      "train-map: tensor(28.8289)\n",
      "val-map: tensor(10.4807)\n",
      "14.910238265991211 val_loss.item()\n",
      "Epoch 21/99\n",
      "----------\n",
      "train-map: tensor(26.3371)\n",
      "val-map: tensor(10.5612)\n",
      "14.95235538482666 val_loss.item()\n",
      "Epoch 22/99\n",
      "----------\n",
      "train-map: tensor(30.7074)\n",
      "val-map: tensor(10.3584)\n",
      "15.074868202209473 val_loss.item()\n",
      "Epoch 23/99\n",
      "----------\n",
      "train-map: tensor(28.2016)\n",
      "val-map: tensor(10.3675)\n",
      "15.240103721618652 val_loss.item()\n",
      "Epoch 24/99\n",
      "----------\n",
      "train-map: tensor(30.5605)\n",
      "val-map: tensor(10.3732)\n",
      "15.323271751403809 val_loss.item()\n",
      "Epoch 25/99\n",
      "----------\n",
      "train-map: tensor(30.7050)\n",
      "val-map: tensor(10.3784)\n",
      "15.535036087036133 val_loss.item()\n",
      "Epoch 26/99\n",
      "----------\n",
      "train-map: tensor(30.2738)\n",
      "val-map: tensor(10.3682)\n",
      "Epoch    26: reducing learning rate of group 0 to 2.5000e-05.\n",
      "15.444930076599121 val_loss.item()\n",
      "Epoch 27/99\n",
      "----------\n",
      "train-map: tensor(34.4823)\n",
      "val-map: tensor(10.3108)\n",
      "15.643641471862793 val_loss.item()\n",
      "Epoch 28/99\n",
      "----------\n",
      "train-map: tensor(33.2814)\n",
      "val-map: tensor(10.2129)\n",
      "15.731000900268555 val_loss.item()\n",
      "Epoch 29/99\n",
      "----------\n",
      "train-map: tensor(32.5157)\n",
      "val-map: tensor(10.1661)\n",
      "15.844157218933105 val_loss.item()\n",
      "Epoch 30/99\n",
      "----------\n",
      "train-map: tensor(32.3205)\n",
      "val-map: tensor(10.1938)\n",
      "15.823614120483398 val_loss.item()\n",
      "Epoch 31/99\n",
      "----------\n",
      "train-map: tensor(35.7927)\n",
      "val-map: tensor(10.1762)\n",
      "15.889532089233398 val_loss.item()\n",
      "Epoch 32/99\n",
      "----------\n",
      "train-map: tensor(34.5230)\n",
      "val-map: tensor(10.1712)\n",
      "16.026575088500977 val_loss.item()\n",
      "Epoch 33/99\n",
      "----------\n",
      "train-map: tensor(32.8984)\n",
      "val-map: tensor(10.1691)\n",
      "15.9913911819458 val_loss.item()\n",
      "Epoch 34/99\n",
      "----------\n",
      "train-map: tensor(34.0735)\n",
      "val-map: tensor(10.1207)\n",
      "16.114669799804688 val_loss.item()\n",
      "Epoch 35/99\n",
      "----------\n",
      "train-map: tensor(37.4586)\n",
      "val-map: tensor(9.9895)\n",
      "Epoch    35: reducing learning rate of group 0 to 1.2500e-05.\n",
      "16.15406608581543 val_loss.item()\n",
      "Epoch 36/99\n",
      "----------\n",
      "train-map: tensor(39.1526)\n",
      "val-map: tensor(10.0741)\n",
      "16.170597076416016 val_loss.item()\n",
      "Epoch 37/99\n",
      "----------\n",
      "train-map: tensor(36.0677)\n",
      "val-map: tensor(10.0742)\n",
      "16.302692413330078 val_loss.item()\n",
      "Epoch 38/99\n",
      "----------\n",
      "train-map: tensor(37.1564)\n",
      "val-map: tensor(10.0953)\n",
      "16.365306854248047 val_loss.item()\n",
      "Epoch 39/99\n",
      "----------\n",
      "train-map: tensor(37.0324)\n",
      "val-map: tensor(10.0415)\n",
      "16.386445999145508 val_loss.item()\n",
      "Epoch 40/99\n",
      "----------\n",
      "train-map: tensor(38.1827)\n",
      "val-map: tensor(9.9393)\n",
      "16.430566787719727 val_loss.item()\n",
      "Epoch 41/99\n",
      "----------\n",
      "train-map: tensor(36.1689)\n",
      "val-map: tensor(9.9948)\n",
      "16.46376609802246 val_loss.item()\n",
      "Epoch 42/99\n",
      "----------\n",
      "train-map: tensor(36.8777)\n",
      "val-map: tensor(9.9479)\n",
      "16.602365493774414 val_loss.item()\n",
      "Epoch 43/99\n",
      "----------\n",
      "train-map: tensor(38.7036)\n",
      "val-map: tensor(9.9479)\n",
      "16.58201789855957 val_loss.item()\n",
      "Epoch 44/99\n",
      "----------\n",
      "train-map: tensor(37.7564)\n",
      "val-map: tensor(9.8950)\n",
      "Epoch    44: reducing learning rate of group 0 to 6.2500e-06.\n",
      "16.623798370361328 val_loss.item()\n",
      "Epoch 45/99\n",
      "----------\n",
      "train-map: tensor(41.2303)\n",
      "val-map: tensor(9.9893)\n",
      "16.59583282470703 val_loss.item()\n",
      "Epoch 46/99\n",
      "----------\n",
      "train-map: tensor(39.7098)\n",
      "val-map: tensor(9.9163)\n",
      "16.766080856323242 val_loss.item()\n",
      "Epoch 47/99\n",
      "----------\n",
      "train-map: tensor(39.8718)\n",
      "val-map: tensor(9.9905)\n",
      "16.674888610839844 val_loss.item()\n",
      "Epoch 48/99\n",
      "----------\n",
      "train-map: tensor(39.1891)\n",
      "val-map: tensor(9.9326)\n",
      "16.660476684570312 val_loss.item()\n",
      "Epoch 49/99\n",
      "----------\n",
      "train-map: tensor(39.6031)\n",
      "val-map: tensor(9.9366)\n",
      "16.73984146118164 val_loss.item()\n",
      "Epoch 50/99\n",
      "----------\n",
      "train-map: tensor(39.6056)\n",
      "val-map: tensor(9.8539)\n",
      "16.76578140258789 val_loss.item()\n",
      "Epoch 51/99\n",
      "----------\n",
      "train-map: tensor(38.0330)\n",
      "val-map: tensor(9.8982)\n",
      "16.834735870361328 val_loss.item()\n",
      "Epoch 52/99\n",
      "----------\n",
      "train-map: tensor(41.6276)\n",
      "val-map: tensor(9.9141)\n",
      "16.885726928710938 val_loss.item()\n",
      "Epoch 53/99\n",
      "----------\n",
      "train-map: tensor(39.9286)\n",
      "val-map: tensor(9.8425)\n",
      "Epoch    53: reducing learning rate of group 0 to 3.1250e-06.\n",
      "16.799388885498047 val_loss.item()\n",
      "Epoch 54/99\n",
      "----------\n",
      "train-map: tensor(43.3272)\n",
      "val-map: tensor(9.8743)\n",
      "16.840200424194336 val_loss.item()\n",
      "Epoch 55/99\n",
      "----------\n",
      "train-map: tensor(40.7933)\n",
      "val-map: tensor(9.9126)\n",
      "16.851837158203125 val_loss.item()\n",
      "Epoch 56/99\n",
      "----------\n",
      "train-map: tensor(39.5048)\n",
      "val-map: tensor(9.8911)\n",
      "16.886093139648438 val_loss.item()\n",
      "Epoch 57/99\n",
      "----------\n",
      "train-map: tensor(40.6545)\n",
      "val-map: tensor(9.9108)\n",
      "16.827001571655273 val_loss.item()\n",
      "Epoch 58/99\n",
      "----------\n",
      "train-map: tensor(42.9374)\n",
      "val-map: tensor(9.8697)\n",
      "16.86400032043457 val_loss.item()\n",
      "Epoch 59/99\n",
      "----------\n",
      "train-map: tensor(37.6999)\n",
      "val-map: tensor(9.8477)\n",
      "16.914701461791992 val_loss.item()\n",
      "Epoch 60/99\n",
      "----------\n",
      "train-map: tensor(40.6274)\n",
      "val-map: tensor(9.8668)\n",
      "16.91374397277832 val_loss.item()\n",
      "Epoch 61/99\n",
      "----------\n",
      "train-map: tensor(37.6467)\n",
      "val-map: tensor(9.8430)\n",
      "16.922441482543945 val_loss.item()\n",
      "Epoch 62/99\n",
      "----------\n",
      "train-map: tensor(43.2136)\n",
      "val-map: tensor(9.8576)\n",
      "Epoch    62: reducing learning rate of group 0 to 1.5625e-06.\n",
      "16.89832305908203 val_loss.item()\n",
      "Epoch 63/99\n",
      "----------\n",
      "train-map: tensor(39.9891)\n",
      "val-map: tensor(9.8647)\n",
      "16.927139282226562 val_loss.item()\n",
      "Epoch 64/99\n",
      "----------\n",
      "train-map: tensor(37.3785)\n",
      "val-map: tensor(9.8430)\n",
      "16.951610565185547 val_loss.item()\n",
      "Epoch 65/99\n",
      "----------\n",
      "train-map: tensor(39.7158)\n",
      "val-map: tensor(9.8380)\n",
      "16.987844467163086 val_loss.item()\n",
      "Epoch 66/99\n",
      "----------\n",
      "train-map: tensor(41.8004)\n",
      "val-map: tensor(9.8405)\n",
      "16.934097290039062 val_loss.item()\n",
      "Epoch 67/99\n",
      "----------\n",
      "train-map: tensor(41.4512)\n",
      "val-map: tensor(9.8417)\n",
      "16.965200424194336 val_loss.item()\n",
      "Epoch 68/99\n",
      "----------\n",
      "train-map: tensor(39.3171)\n",
      "val-map: tensor(9.8349)\n",
      "16.9571475982666 val_loss.item()\n",
      "Epoch 69/99\n",
      "----------\n",
      "train-map: tensor(39.9420)\n",
      "val-map: tensor(9.8422)\n",
      "16.986074447631836 val_loss.item()\n",
      "Epoch 70/99\n",
      "----------\n",
      "train-map: tensor(38.8680)\n",
      "val-map: tensor(9.8255)\n",
      "16.992008209228516 val_loss.item()\n",
      "Epoch 71/99\n",
      "----------\n",
      "train-map: tensor(40.7957)\n",
      "val-map: tensor(9.8294)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    71: reducing learning rate of group 0 to 7.8125e-07.\n",
      "17.010156631469727 val_loss.item()\n",
      "Epoch 72/99\n",
      "----------\n",
      "train-map: tensor(42.0012)\n",
      "val-map: tensor(9.8359)\n",
      "16.99502944946289 val_loss.item()\n",
      "Epoch 73/99\n",
      "----------\n",
      "train-map: tensor(41.5211)\n",
      "val-map: tensor(9.8234)\n",
      "16.983564376831055 val_loss.item()\n",
      "Epoch 74/99\n",
      "----------\n",
      "train-map: tensor(41.2382)\n",
      "val-map: tensor(9.8231)\n",
      "16.990293502807617 val_loss.item()\n",
      "Epoch 75/99\n",
      "----------\n",
      "train-map: tensor(42.6675)\n",
      "val-map: tensor(9.8148)\n",
      "17.008455276489258 val_loss.item()\n",
      "Epoch 76/99\n",
      "----------\n",
      "train-map: tensor(39.2916)\n",
      "val-map: tensor(9.8267)\n",
      "17.02201271057129 val_loss.item()\n",
      "Epoch 77/99\n",
      "----------\n",
      "train-map: tensor(39.8197)\n",
      "val-map: tensor(9.8191)\n",
      "17.005578994750977 val_loss.item()\n",
      "Epoch 78/99\n",
      "----------\n",
      "train-map: tensor(41.0853)\n",
      "val-map: tensor(9.8139)\n",
      "17.016849517822266 val_loss.item()\n",
      "Epoch 79/99\n",
      "----------\n",
      "train-map: tensor(41.2069)\n",
      "val-map: tensor(9.8179)\n",
      "17.025535583496094 val_loss.item()\n",
      "Epoch 80/99\n",
      "----------\n",
      "train-map: tensor(40.6474)\n",
      "val-map: tensor(9.8176)\n",
      "Epoch    80: reducing learning rate of group 0 to 3.9063e-07.\n",
      "17.028881072998047 val_loss.item()\n",
      "Epoch 81/99\n",
      "----------\n",
      "train-map: tensor(41.6471)\n",
      "val-map: tensor(9.8125)\n",
      "17.0327205657959 val_loss.item()\n",
      "Epoch 82/99\n",
      "----------\n",
      "train-map: tensor(40.7343)\n",
      "val-map: tensor(9.8144)\n",
      "17.029197692871094 val_loss.item()\n",
      "Epoch 83/99\n",
      "----------\n",
      "train-map: tensor(38.9176)\n",
      "val-map: tensor(9.8114)\n",
      "17.046674728393555 val_loss.item()\n",
      "Epoch 84/99\n",
      "----------\n",
      "train-map: tensor(44.5514)\n",
      "val-map: tensor(9.8129)\n",
      "17.031566619873047 val_loss.item()\n",
      "Epoch 85/99\n",
      "----------\n",
      "train-map: tensor(42.1639)\n",
      "val-map: tensor(9.8103)\n",
      "17.036483764648438 val_loss.item()\n",
      "Epoch 86/99\n",
      "----------\n",
      "train-map: tensor(38.9356)\n",
      "val-map: tensor(9.8071)\n",
      "17.037612915039062 val_loss.item()\n",
      "Epoch 87/99\n",
      "----------\n",
      "train-map: tensor(40.1664)\n",
      "val-map: tensor(9.8048)\n",
      "17.038043975830078 val_loss.item()\n",
      "Epoch 88/99\n",
      "----------\n",
      "train-map: tensor(38.3214)\n",
      "val-map: tensor(9.8100)\n",
      "17.044086456298828 val_loss.item()\n",
      "Epoch 89/99\n",
      "----------\n",
      "train-map: tensor(39.8033)\n",
      "val-map: tensor(9.8045)\n",
      "Epoch    89: reducing learning rate of group 0 to 1.9531e-07.\n",
      "17.059444427490234 val_loss.item()\n",
      "Epoch 90/99\n",
      "----------\n",
      "train-map: tensor(39.0010)\n",
      "val-map: tensor(9.8049)\n",
      "17.050310134887695 val_loss.item()\n",
      "Epoch 91/99\n",
      "----------\n",
      "train-map: tensor(41.1696)\n",
      "val-map: tensor(9.8054)\n",
      "17.044050216674805 val_loss.item()\n",
      "Epoch 92/99\n",
      "----------\n",
      "train-map: tensor(41.2723)\n",
      "val-map: tensor(9.8029)\n",
      "17.04117202758789 val_loss.item()\n",
      "Epoch 93/99\n",
      "----------\n",
      "train-map: tensor(42.2270)\n",
      "val-map: tensor(9.8067)\n",
      "17.048032760620117 val_loss.item()\n",
      "Epoch 94/99\n",
      "----------\n",
      "train-map: tensor(39.8095)\n",
      "val-map: tensor(9.8035)\n",
      "17.05061912536621 val_loss.item()\n",
      "Epoch 95/99\n",
      "----------\n",
      "train-map: tensor(41.4543)\n",
      "val-map: tensor(9.8022)\n",
      "17.047147750854492 val_loss.item()\n",
      "Epoch 96/99\n",
      "----------\n",
      "train-map: tensor(40.3997)\n",
      "val-map: tensor(9.8043)\n",
      "17.046489715576172 val_loss.item()\n",
      "Epoch 97/99\n",
      "----------\n",
      "train-map: tensor(41.6754)\n",
      "val-map: tensor(9.8033)\n",
      "17.0491886138916 val_loss.item()\n",
      "Epoch 98/99\n",
      "----------\n",
      "train-map: tensor(41.2821)\n",
      "val-map: tensor(9.8056)\n",
      "Epoch    98: reducing learning rate of group 0 to 9.7656e-08.\n",
      "17.056493759155273 val_loss.item()\n",
      "Epoch 99/99\n",
      "----------\n",
      "train-map: tensor(41.9595)\n",
      "val-map: tensor(9.8047)\n",
      "17.052555084228516 val_loss.item()\n",
      "Wall time: 5h 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run([(rgb_model, 0, dataloaders, optimizer, lr_sched, args['comp_info'])], criterion, num_epochs=int(args['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = eval_model([(rgb_model, 0, dataloaders, optimizer, lr_sched, args['comp_info'])], dataloaders['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_:\n",
    "    with open('./output/results.txt', 'w') as file:\n",
    "        file.write(json.dumps(results)) # use `json.loads` to do the reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_:\n",
    "    torch.save(rgb_model, './output/rgb_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
